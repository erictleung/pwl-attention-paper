# pwl-attention-paper
My presentation and distillation of the Attention is All We Need article from Google

## References

Here are some references and resources that I used to help me understand these concepts.

- [SDS 747: Technical Intro to Transformers and LLMs, with Kirill Eremenko](https://www.superdatascience.com/podcast/technical-intro-to-transformers-and-llms-with-kirill-eremenko) - Podcast episode going over brief history and technical details of transformers.
